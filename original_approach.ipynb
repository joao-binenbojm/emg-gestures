{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMG Hand Gesture Classification\n",
    "\n",
    "This notebook is used to explore the approaches from \"A Robust, Real-Time Control Scheme for\n",
    "Multifunction Myoelectric Control\" (Englehart & Hudgins, 2003), while using the data provided by the paper \"Latent Factors Limiting the Performance of sEMG-Interfaces\" (Lobov et al., 2018), recorded using a MYO Thalmic bracelet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import scipy\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "This section serves to take the raw data, and reshape it/preprocess it before any real analysis being carried out.\n",
    "- Data structure: dictionary of lists (keys are subject number), with lists each containing two dataframes corresponding to individual series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '../' # where data directories are found relative to code\n",
    "exclude = [3, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 23, 27, 35] # exclude these subjects due to ADC saturation \n",
    "DIR_SJs = []\n",
    "for idx in range(1, 37):\n",
    "    if idx not in exclude:\n",
    "        current_dir = str(idx)\n",
    "        if idx < 10:\n",
    "            current_dir = '0' + current_dir\n",
    "        DIR_SJs.append(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(DIR, DIR_SJ, series_n):\n",
    "    ''' Given a file path, extracts numerical information based on text file provided. Returns dataframe'''\n",
    "    filename = os.listdir(DIR + DIR_SJ)[series_n] # either the first or second series is extracted\n",
    "    file = open(DIR + DIR_SJ + filename) # open the file to be analysed\n",
    "    lines = file.readlines()\n",
    "    columns = lines[0].split() # take the first row as the column labels for dataframe\n",
    "    lines = lines[1:] # exclude first line which represents column labels\n",
    "    data = np.zeros((len(lines), 10)) # prepare dataframe to be appropriate size\n",
    "    for idx, line in enumerate(lines):\n",
    "        line = line.split() # get entries into different elements        \n",
    "        try:\n",
    "            data[idx, :] = np.array(line).astype(np.float64) # convert to numerical and store in array\n",
    "        except:\n",
    "            print(DIR_SJ, series_n, idx)\n",
    "            print(np.array(line).astype(np.float64), line)\n",
    "    file.close()\n",
    "    for ch in range(1, 9): # interpolate channel values as to make data regularly sampled\n",
    "        f = scipy.interpolate.interp1d(data[:, 0], data[:, ch], fill_value='extrapolate') # create interpolator based on regular data\n",
    "        data[:, ch] = f(list(range(data.shape[0])))\n",
    "    data[:, 0] = list(range(data.shape[0]))\n",
    "    df = pd.DataFrame(data=data, columns=columns)\n",
    "    df[['time', 'class']] = df[['time', 'class']].astype(np.int32) # convert timepoint and class columns to integer type\n",
    "    return df\n",
    "\n",
    "def extract_all_data(DIR, DIR_SJs):\n",
    "    ''' Extracts dataframe from every single subject and series of actions. '''\n",
    "    series_ns = [0,1] # possible series number\n",
    "    all_data = {DIR_SJ:[] for DIR_SJ in DIR_SJs} # initialise dictionary to contain data to be analysed\n",
    "    for DIR_SJ in DIR_SJs:\n",
    "        for series_n in series_ns:\n",
    "            df = parse_file(DIR, DIR_SJ, series_n) # get individual data file\n",
    "            all_data[DIR_SJ].append(df) # add df to the list of that particular subject\n",
    "    return all_data\n",
    "\n",
    "def plot_channels(df):\n",
    "    ''' Plots all channels in a given dataframe '''\n",
    "    plt.figure()\n",
    "    leg = []\n",
    "    for idx in range(1, 9):\n",
    "        plt.plot(df['time'], df['channel{}'.format(idx)])\n",
    "        leg.append('channel{}'.format(idx))\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Voltage (V)')\n",
    "    plt.ylabel('Single Subject Data')\n",
    "    plt.legend(leg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_file(DIR, '01/', 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot signals\n",
    "# for DIR_SJ in DIR_SJs:\n",
    "#     for series_n in range(2):\n",
    "#         print(DIR_SJ, series_n) # determine which signals are definitely not okay to use\n",
    "#         df = parse_file(DIR, DIR_SJ + '/', 0)\n",
    "#         plot_channels(df)\n",
    "\n",
    "# # # Plot class labels\n",
    "# # plt.figure()\n",
    "# # plt.plot(df['time'], df['class'])\n",
    "# # plt.xlabel('Time (ms)')\n",
    "# # plt.ylabel('Voltage (V)')\n",
    "# # plt.ylabel('Single Subject Data')\n",
    "# # plt.show()\n",
    "# df.iloc[:,:-1]ime'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting windows from data\n",
    "- Here, I will create a training and testing set from the first series, then evaluate the majority vote scheme on the second series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureClassifier: \n",
    "\n",
    "    def __init__(self, chosen_model, w=0.250, tau=0.016, pca=False, n_components=10): # assume window length and processing time from original paper\n",
    "        ''' Initialize GestureClassifier object.'''\n",
    "        self.scaler = StandardScaler() # to standardize data\n",
    "        self.feature_scaler = StandardScaler() # to standardize extracted features\n",
    "        self.sos = scipy.signal.butter(N=3, Wn=(10, 499), btype='bandpass', output='sos', fs=1000) # to bandpass filter datas\n",
    "        self.classifier = chosen_model # this way class can be used with multiple different classifiers\n",
    "        self.w = w # window size in m\n",
    "        self.tau = tau # step size in ms\n",
    "        self.fs = 1000 # 1kHz sampling rate\n",
    "        self.do_pca = pca # whether to apply PCA preprocessing or not\n",
    "        self.n_components = n_components # if applying PCA, use this number of components\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        ''' Z-score normalizes channels and bandpasses signals between 10-500Hz (As in Englehart & Hudgins, 2003)'''\n",
    "        self.scaler = self.scaler.fit(df.iloc[:, 1:-1]) # fit scaler\n",
    "        df.iloc[:, 1:-1] = self.scaler.transform(df.iloc[:, 1:-1]) # standardize channels\n",
    "        for idx in range(1, 9): # for each channel, apply bandpass filter\n",
    "            df[['channel{}'.format(idx)]] = scipy.signal.sosfilt(self.sos, df[['channel{}'.format(idx)]])\n",
    "        return df\n",
    "\n",
    "    def zero_crossings(self, win):\n",
    "        ''' Compute zero-crossing count for a given 8-channel window.'''\n",
    "        zero_crossing = np.sum(np.diff(np.sign(win), axis=0).astype(bool), axis=0) # end up wit a row of zero-crossing values\n",
    "        return zero_crossing\n",
    "\n",
    "    def slope_sign_changes(self, win):\n",
    "        ''' Compute slope-sign change count for a given 8-channel window.'''\n",
    "        slope = np.diff(win, axis=0) # get slope estimate\n",
    "        slope_sign_change = np.sum(np.diff(np.sign(slope), axis=0).astype(bool), axis=0) # end up wit a row of zero-crossing values\n",
    "        return slope_sign_change\n",
    "\n",
    "    def waveform_length(self, win):\n",
    "        ''' Computes the waveform length feature for a given 8-channel window.'''\n",
    "        abs_diffs = np.abs(np.diff(win, axis=0)) # compute absolute differences between each set of points for each channels\n",
    "        return abs_diffs.sum(axis=0)\n",
    "\n",
    "    def feature_extraction(self, df):\n",
    "        ''' Given a specific channel of data, applies preprocessing, extracts features, and returns feature matrix '''\n",
    "        df = self.preprocess(df) # apply preprocessing\n",
    "        wsamp = np.around(self.w*self.fs).astype(int) # get window size in number of samples\n",
    "        tausamp = np.around(self.tau*self.fs).astype(int) # get step size in number of samples\n",
    "        wstart = 0 # sample where current window starts\n",
    "        X = np.zeros((0, 32)) # dynamically create feature matrix\n",
    "        y = np.zeros(0) # dynamically create labels vector\n",
    "        while (wstart + wsamp) < df.shape[0]: # while there is still enough data to sample from\n",
    "            x = np.zeros((1, 32)) # where to store features of this specific window (32) features\n",
    "            win, labels = df.iloc[wstart:wstart+wsamp,1:-1], df.loc[wstart:wstart+wsamp,'class'].squeeze()\n",
    "            # print(np.abs(win).mean(axis=0).shape)\n",
    "            # Extract features\n",
    "            x[0, 0:8] = np.abs(win).mean(axis=0) # MAV feature for each channel\n",
    "            x[0, 8:16] = self.zero_crossings(win) # Zero-crossings feature for each channel\n",
    "            x[0, 16:24] = self.slope_sign_changes(win) # Slope-sign changes feature for each channel\n",
    "            x[0, 24:32] = self.waveform_length(win) # Waveform length feature for each channel\n",
    "            X = np.vstack((X, x)) # add new row to matrix\n",
    "            # Choose label for given window\n",
    "            mode,_ = scipy.stats.mode(labels, axis=0, keepdims=False) # find which labels occured the most in the given window\n",
    "            y = np.r_[y, mode] # append the label of the given window to the window of labels\n",
    "            wstart += tausamp # take a step and extract features from next window\n",
    "        # Save data in a new dataframe for processingfrom sklearn.model_selection import train_test_split\n",
    "        fnames = ['MAV', 'ZC', 'SSC', 'WL'] # feature names\n",
    "        columns = [fname+str(idx) for fname in fnames for idx in range(1, 9)] # feature columns\n",
    "        columns.append('class') # add the class label for appropriate column\n",
    "        # self.feature_scaler = self.feature_scaler.fit(X)\n",
    "        # X = self.feature_scaler.transform(X) # standardize extracted features\n",
    "        data = np.hstack((X, y.reshape(-1, 1))) # add the labels column to the rest of the data, with 33 columns in total\n",
    "        df_new = pd.DataFrame(data=data, columns=columns)\n",
    "        return df_new\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' Given extracted features, fit the inputted model.'''\n",
    "        self.feature_scaler = self.feature_scaler.fit(X)\n",
    "        X = self.feature_scaler.transform(X) # standardize extracted features\n",
    "        if self.do_pca: # apply PCA processing if asked for\n",
    "            self.pca = self.pca.fit(X)\n",
    "            X = self.pca.transform(X)\n",
    "        self.classifier = self.classifier.fit(X, y) # fit the chosen model\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Given inputted features, predict.'''\n",
    "        X = self.feature_scaler.transform(X) # standardize extracted features\n",
    "        if self.do_pca:\n",
    "            X = self.pca.transform(X) # if PCA was chosen, apply PCA processing\n",
    "        y_pred = self.classifier.predict(X) # predictions for given data\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that class works (without classifier for now)\n",
    "DIR_SJ, series_n = '01/', 0\n",
    "df = parse_file(DIR, DIR_SJ, series_n)\n",
    "model = GestureClassifier(None) # no chosen model for now, default parameters chosen for the rest\n",
    "df_new = model.feature_extraction(df)\n",
    "df_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization (PCA, LDA, Scatter Plots, etc)\n",
    "This section will serve to visualize the extracted features relative to the classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing class distribution\n",
    "# fig = px.histogram(df_new, x='class', title='Gesture Distribution')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of exctracted features (Variance doesn't drop too slowly here, not so correlated)\n",
    "pca = PCA(n_components=30) # So we can get scree plot of first 30 dims \n",
    "scaler = StandardScaler().fit(df_new.iloc[:,:-1])\n",
    "df_new.iloc[:,:-1] = scaler.transform(df_new.iloc[:,:-1]) # standardize data before PCA\n",
    "pca = pca.fit(df_new.iloc[:,:-1]) # PCA of all columns except class column\n",
    "fig = plt.figure()\n",
    "plt.bar(list(range(1,31)), pca.explained_variance_) # scree plot\n",
    "plt.xlabel('PCs')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Scree Plot of Features')\n",
    "plt.show()\n",
    "fig = plt.figure()\n",
    "plt.bar(list(range(1,31)), np.cumsum(pca.explained_variance_ratio_)) # scree plot\n",
    "plt.xlabel('PCs')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Cumulative Sum of Variances')\n",
    "plt.show()\n",
    "# 3D scatter plot of first 3 PCs\n",
    "pca2 = PCA(n_components=3) # So we can get scree plot of first 30 dims \n",
    "data3d = pca2.fit_transform(df_new.iloc[:,:-1])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colors = ['r', 'g', 'b', 'y', 'm', 'c', 'k']\n",
    "for idx in range(1,7): # for each label except no action\n",
    "    plot_data = data3d[df_new['class']==idx, :]\n",
    "    ax.scatter(plot_data[:,0],plot_data[:,1],plot_data[:,2], c=colors[idx])\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('PCA Reduced Features in 3D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA analysis, followed by LDA classification predictions\n",
    "classifier = LinearDiscriminantAnalysis(n_components=3)\n",
    "classifier = classifier.fit(df_new.iloc[:,:-1], df_new['class'])\n",
    "data3d = classifier.transform(df_new.iloc[:,:-1]) # LDA projection into most separable 3 dimensions\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "colors = ['r', 'g', 'b', 'y', 'm', 'c', 'k']\n",
    "actions = ['unmarked', 'rest', 'fist', 'wrist-flexion', 'wrist-extension', 'radial-deviations', 'ulnar-deviations', 'extended-palm']\n",
    "for idx in range(1,7): # for each label except no action\n",
    "    plot_data = data3d[df_new['class']==idx, :]\n",
    "    ax.scatter(plot_data[:,0],plot_data[:,1],plot_data[:,2], c=colors[idx])\n",
    "ax.set_xlabel('LD1')\n",
    "ax.set_ylabel('LD2')\n",
    "ax.set_zlabel('LDA3')\n",
    "ax.set_title('LDA Reduced Features in 3D')\n",
    "ax.set_label(actions[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Classification Predictions\n",
    "df_new = df_new.drop(df_new[np.isclose(df_new['class'], 0)].index)\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "model = GestureClassifier(classifier) # saves LDA model for classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_new.iloc[:,:-1], df_new['class'], test_size=0.33, random_state=42)\n",
    "model.fit(X_train, y_train) # calling fit method from gesture classifier object\n",
    "y_train_pred, y_test_pred = model.predict(X_train), model.predict(X_test)\n",
    "print('Training Accuracy:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('Testing Accuracy:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassifications seem to be primarily for when the subject is either at rest of performing ulnar deviations\n",
    "pd.DataFrame(data=confusion_matrix(y_test, y_test_pred), columns=actions[1:-1], index=actions[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing performance of trained classifier on second time-series\n",
    "DIR_SJ, series_n = '01/', 1\n",
    "df = parse_file(DIR, DIR_SJ, series_n)\n",
    "df_new = model.feature_extraction(df) # same model as before\n",
    "df_new = df_new.drop(df_new[np.isclose(df_new['class'], 0)].index) # remove any unmarked regions of the data\n",
    "# Dynamic prediction plots\n",
    "pred = model.predict(df_new.iloc[:,:-1]) # predict labels at every row/window\n",
    "df_new['class_pred'] = pred\n",
    "# Plotting results\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(df_new.shape[0]), df_new['class'])\n",
    "plt.scatter(np.arange(df_new.shape[0]), df_new['class_pred'])\n",
    "plt.xlabel('Time(ms)')\n",
    "plt.ylabel('Class Label')\n",
    "plt.title('Classification Dynamics')\n",
    "plt.legend(['True', 'Predicted'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics on new dataset --> translates pretty poorly.\n",
    "print('Testing Accuracy:')\n",
    "print(classification_report(df_new['class'], df_new['class_pred']))\n",
    "pd.DataFrame(data=confusion_matrix(df_new['class'], df_new['class_pred']), columns=actions[1:-1], index=actions[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority voting strategy: For the sake of argument use m decisions (t<=t(now)) from continuous stream to make decision\n",
    "# Implemented as per Englehart and Hudgins, 2003\n",
    "m = 20\n",
    "df_new[['class', 'class_pred']] = df_new[['class', 'class_pred']].astype(np.int32)\n",
    "processed_decisions = np.zeros(df_new.shape[0])\n",
    "processed_decisions[:m] = 1 # assume at rest until enough data in the stream\n",
    "for idx in range(m, df_new.shape[0]):\n",
    "    mode,_ = scipy.stats.mode(df_new.iloc[idx-m:idx+m,-1], axis=0, keepdims=False) # find which labels occured the most in the given window\n",
    "    processed_decisions[idx] = mode # set the decision of that interval to the majority class vote\n",
    "    if idx + 2*m > df_new.shape[0]: # if there are less than m windows left in the last batch\n",
    "        mode,_ = scipy.stats.mode(df_new.iloc[idx+m:,-1], axis=0, keepdims=False) # find which labels occured the most in the given window\n",
    "        processed_decisions[idx:] = 1 # assume at rest at the very end of the stream\n",
    "        break\n",
    "df_new['majority_class'] = processed_decisions # majority decision processed classes\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(df_new.shape[0]), df_new['class'])\n",
    "plt.scatter(np.arange(df_new.shape[0]), df_new['majority_class'])\n",
    "plt.xlabel('Time(ms)')\n",
    "plt.ylabel('Class Label')\n",
    "plt.title('Classification Dynamics')\n",
    "plt.legend(['True', 'Predicted (Processed)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics on new dataset --> accuracy barely improves with majority vote\n",
    "print('Testing Accuracy:')\n",
    "print(classification_report(df_new['class'], df_new['majority_class']))\n",
    "pd.DataFrame(data=confusion_matrix(df_new['class'], df_new['majority_class']), columns=actions[1:-1], index=actions[1:-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-subject performance!\n",
    "This section will train the classifier on a subset of subjects and evaluate performane across the remaining subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GestureClassifier(LinearDiscriminantAnalysis())\n",
    "for k, DIR_SJ in tqdm(enumerate(DIR_SJs)): # for each subject\n",
    "    for series_n in range(2): # for each series of each subject\n",
    "        df = parse_file(DIR, DIR_SJ+'/', series_n)\n",
    "        df_new = model.feature_extraction(df) # same model as before\n",
    "        df_new = df_new.drop(df_new[np.isclose(df_new['class'], 0)].index) # remove any unmarked regions of the data\n",
    "        df_new['sub'] = [DIR_SJ for idx in range(df_new.shape[0])] # so we keep track of which windows are marked per subject\n",
    "        if k == 0 and series_n == 0: # if the first batch of data extracted, save it to a new dataframe\n",
    "            df_all = df_new\n",
    "        else:\n",
    "            df_all = pd.concat((df_all, df_new), ignore_index=True, axis=0) # add new batch of data to main data collection\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed-subjects and trials (but training on a bit of all subjects), seems to yield okay performance across subjects\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "model = GestureClassifier(classifier) # saves LDA model for classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_all.iloc[:,:-2], df_all['class'], test_size=0.33)\n",
    "model.fit(X_train, y_train) # calling fit method from gesture classifier object\n",
    "y_train_pred, y_test_pred = model.predict(X_train), model.predict(X_test)\n",
    "print('Training Accuracy:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('Testing Accuracy:')\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on subset of subjects and evaluating on the rest of subjects (held-out subjects)\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "model = GestureClassifier(classifier) # saves LDA model for classification\n",
    "subs = df_all['sub'].unique()\n",
    "test_subjects = np.random.choice(subs, size=np.around(len(subs)*0.33).astype(np.int32), replace=False) # pick 1/3 of subjects and randomly add them to test set\n",
    "df_test = df_all[df_all['sub'].isin(test_subjects)] # also shuffles test set\n",
    "df_train = df_all[~df_all['sub'].isin(test_subjects)].sample(frac=1) # if not in test_subjects, is used for training and shuffled\n",
    "model.fit(df_train.iloc[:,:-2], df_train['class']) # calling fit method from gesture classifier object\n",
    "y_train_pred, y_test_pred = model.predict(df_train.iloc[:,:-2],), model.predict(df_test.iloc[:,:-2],)\n",
    "print('Training Accuracy:')\n",
    "print(classification_report(df_train['class'], y_train_pred))\n",
    "print('Testing Accuracy:')\n",
    "print(classification_report(df_test['class'], y_test_pred))\n",
    "pd.DataFrame(data=confusion_matrix(df_test['class'], y_test_pred), columns=actions[1:], index=actions[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new column to test\n",
    "df_test['class_pred'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority voting strategy: For the sake of argument use m decisions (t<=t(now)) from continuous stream to make decision\n",
    "# Implemented as per Englehart and Hudgins, 2003\n",
    "m = 10\n",
    "df_test[['class', 'class_pred']] = df_test[['class', 'class_pred']].astype(np.int32)\n",
    "processed_decisions = np.zeros(df_test.shape[0])\n",
    "processed_decisions[:m] = 1 # assume at rest until enough data in the stream\n",
    "for idx in range(m, df_test.shape[0]):\n",
    "    mode,_ = scipy.stats.mode(df_test.iloc[idx-m:idx+m,-1], axis=0, keepdims=False) # find which labels occured the most in the given window\n",
    "    processed_decisions[idx] = mode # set the decision of that interval to the majority class vote\n",
    "    if idx + 2*m > df_test.shape[0]: # if there are less than m windows left in the last batch\n",
    "        mode,_ = scipy.stats.mode(df_test.iloc[idx+m:,-1], axis=0, keepdims=False) # find which labels occured the most in the given window\n",
    "        processed_decisions[idx:] = 1 # assume at rest at the very end of the stream\n",
    "        break\n",
    "df_test['majority_class'] = processed_decisions # majority decision processed classes\n",
    "# df_test.reset_index(inplace=True)\n",
    "plt.figure()\n",
    "# plt.plot(np.arange(df_test.shape[0]), df_test['class'])\n",
    "# plt.plot(np.arange(df_test.shape[0]), df_test['majority_class'])\n",
    "plt.plot(np.arange(10000), df_test.loc[:9999,'class'])\n",
    "plt.plot(np.arange(10000), df_test.loc[:9999,'majority_class'])\n",
    "plt.xlabel('Time(ms)')\n",
    "plt.ylabel('Class Label')\n",
    "plt.title('Classification Dynamics')\n",
    "plt.legend(['True', 'Predicted (Processed)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Accuracy:')\n",
    "print(classification_report(df_test['class'], df_test['majority_class']))\n",
    "pd.DataFrame(data=confusion_matrix(df_test['class'], df_test['majority_class']), columns=actions[1:], index=actions[1:])\n",
    "df_test.drop(columns='majority_class', inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture-env",
   "language": "python",
   "name": "gesture-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8069f27074d77f25199843d450049c3042ccfc51f948861a6ea2b6b0163380aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
